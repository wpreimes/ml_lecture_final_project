"""
def scale_all(df):
    df['Total precipitation'] = min_max_scale(df['Total precipitation'], 0, 1)
    # Soil moisture is scaled between 0.1 and 1
    df['reanalysis SM'] = min_max_scale(df['reanalysis SM'], 0.1, 1)
    # Soil moisture gaps are assigned 0
    df['reanalysis SM'] = df['reanalysis SM'].fillna(0)
    
    # PET as z-scores
    mean = df['Potential evaporation'].mean()
    std = df['Potential evaporation'].std()
    df['Potential evaporation'] = (df['Potential evaporation'] - mean) / std
    
    mean = df['Skin temperature'].mean()
    std = df['Skin temperature'].std()
    df['Skin temperature'] = (df['Skin temperature'] - mean) / std
    return df
"""

class LSTMModel(nn.Module):
    def __init__(self, n_features, hidden_size=64):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_size,
            batch_first=True
        )
        nn.Bilinear(hidden_size)
        nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x: (batch, window, features)
        out, (h_n, c_n) = self.lstm(x)
        # Use last hidden state
        out = h_n[-1]
        out = self.fc(out)
        return out

# LSTM Prediction
import os
#os.environ['TF_DETERMINISTIC_OPS'] = '1'  # optional for GPU determinism
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # forces CPU
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler

tf.random.set_seed(123)
np.random.seed(123)

#df = pd.concat([DATA[['reana']], DATA[utils.pretty_vars]], axis=1)
df = DATA.copy()
df['train'] = PREDICTION['train'].dropna()


def make_windows(df, features, target, window):
    """
    One sided window at the target time stamp
    """
    X, y = [], []

    for i in range(WINDOW, len(df)):
        X.append(df[features].iloc[i-window:i].values)
        y.append(df[target].iloc[i])

    return np.array(X), np.array(y)
    
WINDOW = 60  # one sided days  todo: fine-tune
features = utils.pretty_vars#  + ['reanalysis SM']
# add feature DOY
X, y = make_windows(df, 
                    features=features,
                    target='reanalysis SM', 
                    window=WINDOW)

#X = StandardScaler().fit_transform(X.reshape(-1, X.shape[2])).reshape(X.shape)
#scaler_y = StandardScaler()
#y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

# Remove samples where target is missing (during training)
#valid = ~np.isnan(y)
#X = X[valid]
#y = y[valid]

model = Sequential([
    Input(shape=(WINDOW, X.shape[2])),
    LSTM(64, return_sequences=False, recurrent_activation='sigmoid'),   # input_shape=(WINDOW, X.shape[2])
    Dense(1)
])

model.compile(
    optimizer="adam",
    loss="mse"    # masked mean square error?
)

print(model.summary())


early_stop = EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)

pbar = TqdmEpochProgress()

history = model.fit(
    X,
    y,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    shuffle=False,
    callbacks=[early_stop, pbar],  #,
    verbose=1
)
print("Training done")



PREDICTION["filled_lstm"] = np.nan

# predict multiple days / window -> take average
for i in tqdm(range(WINDOW, len(PREDICTION.index))):
    if bool(PREDICTION["gaps"].iloc[i]) is True:
        x = df[features].iloc[i-WINDOW:i].values[np.newaxis, ...]
        PREDICTION.loc[PREDICTION.index[i], "filled_lstm"] = model.predict(x, verbose=0)[0, 0]

#PREDICTION["filled_lstm"] = scaler_y.inverse_transform(PREDICTION["filled_lstm"].values.reshape(-1,1)).flatten()
print("done")
